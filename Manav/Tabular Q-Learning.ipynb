{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/34/0a311fedffcc7a153bbc0390ef4c378dbc7f09f9865247137f82d62f8e7a/cmake-3.15.3-py3-none-manylinux2010_x86_64.whl (16.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.5MB 933kB/s ta 0:00:011    69% |██████████████████████▏         | 11.5MB 4.2MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting gym[atari]\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 2.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/manav/anaconda3/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/manav/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.16.2)\n",
      "Requirement already satisfied: six in /home/manav/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.12.0)\n",
      "Collecting pyglet<=1.3.2,>=1.2.0 (from gym[atari])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 4.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle~=1.2.0 (from gym[atari])\n",
      "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Collecting opencv-python (from gym[atari])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/38/60de02a4c9013b14478a3f681a62e003c7489d207160a4d7df8705a682e7/opencv_python-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (28.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 28.3MB 1.0MB/s ta 0:00:011   27% |█████████                       | 7.9MB 5.1MB/s eta 0:00:05    32% |██████████▍                     | 9.2MB 13.5MB/s eta 0:00:02    82% |██████████████████████████▍     | 23.4MB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting atari_py~=0.2.0 (from gym[atari])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 862kB/s ta 0:00:01  2% |▉                               | 71kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/manav/anaconda3/lib/python3.7/site-packages (from gym[atari]) (6.2.0)\n",
      "Requirement already satisfied: future in /home/manav/anaconda3/lib/python3.7/site-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.17.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/manav/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
      "Successfully built gym\n",
      "\u001b[31mspyder 3.3.3 requires pyqt5<=5.12; python_version >= \"3\", which is not installed.\u001b[0m\n",
      "Installing collected packages: cmake, pyglet, cloudpickle, opencv-python, atari-py, gym\n",
      "  Found existing installation: cloudpickle 0.8.0\n",
      "    Uninstalling cloudpickle-0.8.0:\n",
      "      Successfully uninstalled cloudpickle-0.8.0\n",
      "Successfully installed atari-py-0.2.6 cloudpickle-1.2.2 cmake-3.15.3 gym-0.15.4 opencv-python-4.1.2.30 pyglet-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action SpaceDiscrete(6)\n",
      "State SpaceDiscrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action Space{}\".format(env.action_space))\n",
    "print(\"State Space{}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state=env.encode(3,1,2,0)\n",
    "print(\"State:\",state)\n",
    "env.s=state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1952\n",
      "Penalties incurred: 595\n"
     ]
    }
   ],
   "source": [
    "# Without RL\n",
    "env.s=328\n",
    "epochs=0\n",
    "penalties, reward=0,0\n",
    "frames=[]\n",
    "done=False\n",
    "while not done:\n",
    "    action=env.action_space.sample()\n",
    "    state,reward,done,info=env.step(action)\n",
    "    if reward==-10:\n",
    "        penalties+=1\n",
    "    frames.append({'frame':env.render(mode='ansi'),\n",
    "                  'state':state,\n",
    "                  'action':action,\n",
    "                  'reward':reward})\n",
    "    epochs+=1\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1952\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def print_frames(frames):\n",
    "    for i,frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        #print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i+1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Q-Learning\n",
    "import numpy as np\n",
    "q_table=np.zeros([env.observation_space.n,env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finsihed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import clear_output\n",
    "#Hyperparameters\n",
    "alpha=0.1\n",
    "gamma=0.6\n",
    "epsilon=0.1\n",
    "\n",
    "#for plotting metrics\n",
    "all_epochs=[]\n",
    "all_penalties=[]\n",
    "\n",
    "for i in range(1,100001):\n",
    "    state=env.reset()\n",
    "    epochs,penalties,reward=0,0,0\n",
    "    done=False\n",
    "    while not done:\n",
    "        if random.uniform(0,1)<epsilon:\n",
    "            action=env.action_space.sample()\n",
    "        else:\n",
    "            action=np.argmax(q_table[state])\n",
    "        next_state, reward, done, info=env.step(action)\n",
    "        \n",
    "        old_value=q_table[state,action]\n",
    "        next_max=np.max(q_table[next_state])\n",
    "        new_value=(1-alpha)*old_value+alpha*(reward+gamma*next_max)\n",
    "        q_table[state,action]=new_value\n",
    "        \n",
    "        if reward==-10:\n",
    "            penalties+=1\n",
    "        state=next_state\n",
    "        epochs+=1\n",
    "        \n",
    "    if i%100==0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.39822645,  -2.27325184,  -2.40702552,  -2.3588204 ,\n",
       "       -10.31327957, -10.06746382])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.74\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties= 0,0\n",
    "episodes=100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state=env.reset()\n",
    "    epochs,penalties,reward=0,0,0\n",
    "    done=False\n",
    "    while not done:\n",
    "        action=np.argmax(q_table[state])\n",
    "        state,reward,done,info=env.step(action)\n",
    "        if reward==-10:\n",
    "            penalties+=1\n",
    "        epochs+=1\n",
    "    \n",
    "    total_penalties+=penalties\n",
    "    total_epochs+=epochs\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
