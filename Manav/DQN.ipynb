{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Source: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\\n In this task, rewards are +1 for every incremental timestep and the environment terminates\\n if the pole falls over too far or the cart moves more then 2.4 units away from center'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Source: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    " In this task, rewards are +1 for every incremental timestep and the environment terminates\n",
    " if the pole falls over too far or the cart moves more then 2.4 units away from center\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conda.compat module is deprecated and will be removed in a future release.\n",
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/manav/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.1\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _libgcc_mutex-0.1          |             main           3 KB\n",
      "    conda-4.8.0                |           py37_1         3.0 MB\n",
      "    conda-package-handling-1.6.0|   py37h7b6447c_0         872 KB\n",
      "    cudatoolkit-10.1.243       |       h6bb024c_0       513.2 MB\n",
      "    ninja-1.9.0                |   py37hfd86e86_0         1.6 MB\n",
      "    pytorch-1.3.1              |py3.7_cuda10.1.243_cudnn7.6.3_0       428.0 MB  pytorch\n",
      "    torchvision-0.4.2          |       py37_cu101        14.9 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       961.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
      "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0\n",
      "  ninja              pkgs/main/linux-64::ninja-1.9.0-py37hfd86e86_0\n",
      "  pytorch            pytorch/linux-64::pytorch-1.3.1-py3.7_cuda10.1.243_cudnn7.6.3_0\n",
      "  torchvision        pytorch/linux-64::torchvision-0.4.2-py37_cu101\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                                       4.6.11-py37_0 --> 4.8.0-py37_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.8.0          | 3.0 MB    | ##################################### | 100% \n",
      "ninja-1.9.0          | 1.6 MB    | ##################################### | 100% \n",
      "conda-package-handli | 872 KB    | ##################################### | 100% \n",
      "cudatoolkit-10.1.243 | 513.2 MB  | ##################################### | 100% \n",
      "torchvision-0.4.2    | 14.9 MB   | ##################################### | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \n",
      "pytorch-1.3.1        | 428.0 MB  | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision cudatoolkit=10.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\\n    ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "    ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state','action','next_state','reward'))\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "    def push(self,*args):\n",
    "        #Saves a transtion\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position+1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "            return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,h,w,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,16,kernel_size=5,stride=2)\n",
    "        self.bn1=nn.BatchNorm2d(16)\n",
    "        self.conv2=nn.Conv2d(16,32,kernel_size=5,stride=2)\n",
    "        self.bn2=nn.BatchNorm2d(32)\n",
    "        self.conv3=nn.Conv2d(32,32,kernel_size=5, stride=2)\n",
    "        self.bn3=nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size,kernel_size=5, stride=2):\n",
    "            return (size-(kernel_size-1) - 1)//stride + 1\n",
    "        convw=conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh=conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size=convw*convh*32\n",
    "        self.head=nn.Linear(linear_input_size,outputs)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.bn1(self.conv1(x)))\n",
    "        x=F.relu(self.bn2(self.conv2(x)))\n",
    "        x=F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize=T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()])\n",
    "def get_cart_location(screen_width):\n",
    "    world_width=env.x_threshold*2\n",
    "    scale=screen_width/world_width\n",
    "    return int(env.state[0]*scale+screen_width/2.0)\n",
    "def get_screen():\n",
    "    screen=env.render(mode='rgb_array').transpose((2,0,1))\n",
    "    _,screen_height,screen_width=screen.shape\n",
    "    screen=screen[:,int(screen_height*.4):int(screen_height*.8)]\n",
    "    view_width=int(screen_width*.6)\n",
    "    cart_location=get_cart_location(screen_width)\n",
    "    if cart_location<view_width//2:\n",
    "        slice_range=slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range=slice(-view_width,None)\n",
    "    else:\n",
    "        slice_range=slice(cart_location-view_width//2, cart_location+view_width//2)\n",
    "    screen=screen[:,:,slice_range]\n",
    "    screen=np.ascontiguousarray(screen, dtype=np.float32)/255\n",
    "    screen=torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1,2,0).numpy(),interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "init_screen=get_screen()\n",
    "_,_,screen_height,screen_width=init_screen.shape\n",
    "n_actions=env.action_space.n\n",
    "policy_net=DQN(screen_height,screen_width,n_actions).to(device)\n",
    "target_net=DQN(screen_height,screen_width,n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer=optim.RMSprop(policy_net.parameters())\n",
    "memory=ReplayMemory(10000)\n",
    "\n",
    "steps_done=0\n",
    "\n",
    "def  select_action(state):\n",
    "    global steps_done\n",
    "    sample=random.random()\n",
    "    eps_threshold =EPS_END + (EPS_START - EPS_END)*\\\n",
    "        math.exp(-1.*steps_done/EPS_DECAY)\n",
    "    steps_done+=1\n",
    "    if sample>eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]],device=device,dtype=torch.long)\n",
    "episode_durations=[]\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t=torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training..')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    if len(durations_t)>=100:\n",
    "        means=durations_t.unfold(0,100,1).mean(1).view(-1)\n",
    "        means=torch.cat((torch.zeros(99),means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(.001)\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory)<BATCH_SIZE:\n",
    "        return\n",
    "    transitions=memory.sample(BATCH_SIZE)\n",
    "    #This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask=torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states=torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch=torch.cat(batch.state)\n",
    "    action_batch=torch.cat(batch.action)\n",
    "    reward_batch=torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values=policy_net(state_batch).gather(1,action_batch)\n",
    "    \n",
    "    next_state_values=torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask]=target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values=(next_state_values*GAMMA)+reward_batch\n",
    "    \n",
    "    loss=F.smooth_l1_loss(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes=50\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    last_screen=get_screen()\n",
    "    current_screen=get_screen()\n",
    "    state=current_screen-last_screen\n",
    "    for t in count():\n",
    "        action=select_action(state)\n",
    "        _,reward,done,_=env.step(action.item())\n",
    "        reward=torch.tensor([reward],device=device)\n",
    "        \n",
    "        last_screen=current_screen\n",
    "        current_screen=get_screen()\n",
    "        if not done:\n",
    "            next_state=current_screen-last_screen\n",
    "        else:\n",
    "            next_state=None\n",
    "            \n",
    "        memory.push(state,action,next_state,reward)\n",
    "        \n",
    "        state=next_state\n",
    "        \n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t+1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
